{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0aa993a",
   "metadata": {},
   "source": [
    "## Application procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7836ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.neighbors import NearestNeighbors\n",
    "\n",
    "def main():\n",
    "    dataset = generate_data()\n",
    "    \n",
    "    nn = NearestNeighbors()\n",
    "    \n",
    "    nn.fit(dataset)\n",
    "    \n",
    "    points = [point1, point2, point3]\n",
    "    while not converged:\n",
    "        points = nn.kneighbors(points, n_neighbors=5)\n",
    "        process_points(points)\n",
    "        evaluate_next_points()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b6e4d",
   "metadata": {},
   "source": [
    "## Current implementation on dislib\n",
    "\n",
    "This is an abstraction for easy understanding. Actual code is more convoluted, but general flow and graph of tasks is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4746aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighbors(BaseEstimator):\n",
    "    def fit(self, x):\n",
    "        self._fit_data = x\n",
    "        return self\n",
    "    \n",
    "    def kneighbors(self, points, ...):\n",
    "        partial_results = list()\n",
    "        \n",
    "        for row_blocks in self._fit_data.iterator(axis=\"rows\"):\n",
    "            # Following lines are enclosed in a task\n",
    "            nn = sklearn.NearestNeighbors()\n",
    "            nn.fit(row_blocks)\n",
    "            partial_results.append(nn.kneighbors(points))\n",
    "        \n",
    "        # This is a task\n",
    "        return _merge(partial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5bfe5a",
   "metadata": {},
   "source": [
    "## Correct IMHO implementation on dislib\n",
    "\n",
    "Somebody may want to implement this. If implemented, this can become the baseline for our evaluation, otherwise we have no baseline to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def fit_task(row_blocks)\n",
    "    nn = sklearn.NearestNeighbors()\n",
    "    nn.fit(row_blocks)\n",
    "    return nn\n",
    "\n",
    "class NearestNeighbors(BaseEstimator):\n",
    "    def fit(self, x):\n",
    "        self._preprocessed_blocks = list()\n",
    "        \n",
    "        for row_blocks in x.iterator(axis=\"rows\"):\n",
    "            nn = fit_task(row_blocks)\n",
    "            self._preprocessed_blocks.append(nn)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def kneighbors(self, points, ...):\n",
    "        partial_results = list()\n",
    "        \n",
    "        for sk_pp_block in self._preprocessed_blocks:\n",
    "            # Following lines should be enclosed into a task \n",
    "            partial_results.append(sk_pp_block.kneighbors(points))\n",
    "        \n",
    "        # This is also a task\n",
    "        return _merge(partial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7be04f",
   "metadata": {},
   "source": [
    "## Intermission: index mapping\n",
    "\n",
    "`sklearn.NearestNeighbors.kneighbors` returns the indexes of the points for the input dataset. In this notebook and in general, we want to get the indexes for the full dataset. We are giving `sklearn` a portion of the dataset, so indexes will be _local_ and not _global_.\n",
    "\n",
    "The current merge implementation on COMPSs relies on tracking the _index offset_ by keeping track on the number of points processed into the query (the list that goes into the merge follows the original order).\n",
    "\n",
    "The following examples have explicit index translation table. Previous examples also have some index translation, but it not shown. It is quite simple, but it is required nevertheless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ac580",
   "metadata": {},
   "source": [
    "## Draft proposal with split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NearestNeighborsWithSplit(BaseEstimator):\n",
    "    def fit(self, x):\n",
    "        self._preprocessed_blocks = list()\n",
    "        self._index_translation_table = list()\n",
    "\n",
    "        for partition in split(x):\n",
    "            # Following lines should be enclosed into a task\n",
    "            # -------------------------\n",
    "            subdataset = np.stack(partition._chunks)\n",
    "            nn = sklearn.NearestNeighbors()\n",
    "            nn.fit(subdataset)\n",
    "            \n",
    "            # This is to be discussed\n",
    "            itt = np.zeros(len(subdataset), dtype=int)\n",
    "            n = 0\n",
    "            for row_block in partition._chunks:\n",
    "                self._index_translation_table[n:n + len(row_block)] = \\\n",
    "                    range(row_block.offset, row_block.offset + len(row_block))\n",
    "                                  #********#\n",
    "                                  # This relies on PersistentBlock.offset attribute\n",
    "            # -------------------------\n",
    "            \n",
    "            self._preprocessed_blocks.append(nn)\n",
    "            self._index_translation_table.append(itt)\n",
    "\n",
    "    def kneighbors(self, points, ...):\n",
    "        partial_results = list()\n",
    "\n",
    "        for sk_pp_block in self._preprocessed_blocks:\n",
    "            # Following lines should be enclosed into a task \n",
    "            partial_results.append(sk_pp_block.kneighbors(points))\n",
    "\n",
    "        # Now perform the remapping. Note that partial_results follows the same order\n",
    "        # as self._index_translation_table, so remapping is immediate\n",
    "        return _merge(partial_results, self._index_translation_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e3ca5",
   "metadata": {},
   "source": [
    "# Appendix A\n",
    "\n",
    "## Alternatives for index translation table\n",
    "\n",
    "The previous example showed the use of `PersistentBlock.offset` in order to get the offset. This is the most generic approach in which matrixes may be irregular in terms of number-of-points-per-block, and the Block self-contains the offset for each block (this information can be considered redundant from the point of view of the dislib array).\n",
    "\n",
    "If adding an offset attribute to PersistentBlock is not desirable, a similar result can be achieved by relying on regularness of the input dataset dislib array. Note that regularness of dislib array is not generally a requirement.\n",
    "\n",
    "Adding a `split` method on the dislib array with an implementation that relies on the `split` dataClay general idea for its blocks while keeping track of the index is also a idea. This would result in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArraySplit(object):\n",
    "    ...\n",
    "\n",
    "class Array(object):\n",
    "    def split(self, axis=0):\n",
    "        ret = list()\n",
    "        # < iterate chunks >\n",
    "        for ... in ...:\n",
    "            # < assign chunks to some substructures >\n",
    "            # < build an array for each thing >\n",
    "            arr = Array()\n",
    "            # < keep track of index >\n",
    "            offset = [m, n]\n",
    "            \n",
    "            # Completely incorrect Python code, but the main idea remains\n",
    "            ArraySplit.append(arr, offset)\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "# In the main application\n",
    "for partition in x.split(\"rows\"):\n",
    "    # partition is an ArraySplit instance\n",
    "    for array, offset in partition.iter_with_offset():\n",
    "        # do stuff here\n",
    "        ...\n",
    "\n",
    "# For applications that do not care on the offsets:\n",
    "for partition in x.split(\"rows\"):\n",
    "    for array in partition:\n",
    "        # do stuff here\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca1c0c",
   "metadata": {},
   "source": [
    "# Appendix B\n",
    "\n",
    "## Huge `points` considerations\n",
    "\n",
    "The `points` structure passed to the `kneighbors` call may be a small list but it can potentially be a huge one. The first use case is related to things such as _edge detection_ that require multiple calls and build upon previous results. The second use case is related to clusterization or feature extraction.\n",
    "\n",
    "A `kneighbors` implementation that is ready for this use case is conceivable, albeit I would avoid it in the first proof-of-concept implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighborsWithSplit(BaseEstimator):\n",
    "    def kneighbors(self, points, ...):\n",
    "        if is_a_dislib_array(points) and has_more_than_one_row_block(points):\n",
    "            for partition in split(points):\n",
    "                # Following lines should be enclosed into a task\n",
    "                # -------------------------\n",
    "\n",
    "                # AFAICT, there is no real benefit to stack points into a single block\n",
    "                # so we can safely make the iteration inside the task\n",
    "                for query_index, query_chunk in zip(partition.get_indexes(), partition._chunks):\n",
    "                    r = self.kneighbors(query_chunk)  # do not fear, this is NOT a recursive algorithm\n",
    "                    partial_results.append(query_index, r)\n",
    "                    \n",
    "            sort_and_join(partial_results)\n",
    "        else:\n",
    "            # the simpler code in previous examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e425075",
   "metadata": {},
   "source": [
    "# Update 2021/11/25\n",
    "\n",
    "## Encapsulating the index tracking semantic in an agnostic-ish way\n",
    "\n",
    "Up until now, we had the `split` concept, alongside a `GenericSplit` for simple stuff (as well as some prospections for `WorkStealingSplit` and similar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current state\n",
    "\n",
    "for partition in split(experiment, split_class=GenericSplit):\n",
    "    # each partition is an instance of GenericSplit \n",
    "    # (registered class, built-in, provided by dataClay)\n",
    "    partition._chunks  # <- this gives a list of chunks\n",
    "    for chunk in partition:  # <- this is transparent way to access those same chunks\n",
    "        pass\n",
    "    partition.get_chunkindexes()  # <- this gives the position of each **chunk**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09de915",
   "metadata": {},
   "source": [
    "My idea is to build upon that and have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a282cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in split(experiment, split_class=SplitWithIndexTrackingCoordinator()):\n",
    "    # each partition is an instance of SplitWithIndexTracking\n",
    "    # and they are coordinated by the SplitWithIndexTrackingCoordinator\n",
    "    # (all registered classes, built-in, provided by dataClay)\n",
    "    \n",
    "    # Existing features (\"inherited\" or whatever)\n",
    "    partition.get_chunkindexes()\n",
    "    \n",
    "    # New features of this SplitWITC:\n",
    "    partition.get_itemindexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aceada",
   "metadata": {},
   "source": [
    "The only requirement for SplitWithIndexTrackingCoordinator is that each element should have the concept of _length_ (which, in Python, it means that `len(object)` should work and typically this is achieved through the implementation of `__len__` method).\n",
    "\n",
    "This is satisfied by the `PersistentBlock` of the dislib array, but it is quite generic and expected. So it can be used by most data structures.\n",
    "\n",
    "**Semantic nitpicking / language-lawyer-mode-on:** `split_class` should be renamed to `split_factory`. But otherwise, it is a backwards-compatible change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053efbf",
   "metadata": {},
   "source": [
    "### How does this work?\n",
    "\n",
    "The `split` procedure uses the `SplitClass.add_object` method. For `SplitWithIndexTracking` instances, that will trigger a call to `len` and update a global index counter on `SplitWithIndexTrackingCoordinator` as well as a local index mapping table for each split.\n",
    "\n",
    "It sounds convoluted, but\n",
    "\n",
    " - this basic strategy resembles the one used by the dislib\n",
    " - it is compatible with all `len`-supporting structures (which are most)\n",
    " - the idea of this `SplitXXXCoordinator` can be reused for other situations\n",
    " \n",
    " \n",
    "## Draft proposal with this idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1eaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighborsWithSplit(BaseEstimator):\n",
    "    def fit(self, x):\n",
    "        self._preprocessed_blocks = list()\n",
    "        self._index_translation_table = list()\n",
    "\n",
    "        for partition in split(x, split_factory=SplitWITC()):\n",
    "            # Following lines should be enclosed into a task\n",
    "            # -------------------------\n",
    "            subdataset = np.stack(partition._chunks)\n",
    "            nn = sklearn.NearestNeighbors()\n",
    "            nn.fit(subdataset)\n",
    "            \n",
    "            self._preprocessed_blocks.append(nn)  # nn is a sklearn\n",
    "            self._index_translation_table.append(partition.get_itemindexes())\n",
    "                                                         # *******************\n",
    "                                                         # this is provided by SplitWITC()\n",
    "            # type of idx_local_to_global is open to discussion\n",
    "            # but the semantic is clear: \n",
    "            # it conveys a way to convert indexes from local to global\n",
    "\n",
    "    def kneighbors(self, points, ...):\n",
    "        partial_results = list()\n",
    "\n",
    "        for sk_pp_block, itt in zip(self._preprocessed_blocks, self._index_translation_table):\n",
    "            # Following lines should be enclosed into a task \n",
    "            distances, indexes = sk_pp_block.kneighbors(points)\n",
    "            partial_results.append(distances, itt[indexes])\n",
    "\n",
    "        # Now perform the merge.\n",
    "        # This is now simpler than before because index are global\n",
    "        # so the code complexity is reduced\n",
    "        return _merge(partial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d533be",
   "metadata": {},
   "source": [
    "### Comparison between `merge` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dislib version with index tracking stuff\n",
    "@task(returns=2)\n",
    "def _merge(*queries):\n",
    "    final_dist, final_ind, offset = queries[0]\n",
    "\n",
    "    for dist, ind, n_samples in queries[1:]:\n",
    "        ind += offset\n",
    "        offset += n_samples\n",
    "\n",
    "        # keep the indices of the samples that are at minimum distance\n",
    "        m_ind = _min_indices(final_dist, dist)\n",
    "        comb_ind = np.hstack((final_ind, ind))\n",
    "\n",
    "        final_ind = np.array([comb_ind[i][m_ind[i]]\n",
    "                              for i in range(comb_ind.shape[0])])\n",
    "\n",
    "        # keep the minimum distances\n",
    "        final_dist = _min_distances(final_dist, dist)\n",
    "\n",
    "    return final_dist, final_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split version (untested)\n",
    "@task(returns=2)\n",
    "def _merge(*queries):\n",
    "    final_dist, final_ind = zip(*queries)\n",
    "    \n",
    "    # Stack everything\n",
    "    final_dist = np.hstack(final_dist)\n",
    "    final_ind = np.hstack(final_ind)\n",
    "    \n",
    "    # And now sort distances and keep the resulting first n\n",
    "    result_ind = np.argsort(final_dist)[:\"\"\"number of points to keep\"\"\"]\n",
    "    \n",
    "    return final_dist[result_ind], final_ind[result_ind]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
