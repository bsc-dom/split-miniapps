{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(10, 5)})\n",
    "\n",
    "from tad4bj import DataStorage\n",
    "\n",
    "EXPERIMENT_SIZE_FIELDS = [\n",
    "    \"number_of_fragments\",\n",
    "    \"points_per_fragment\", \n",
    "]\n",
    "\n",
    "PLATFORM_FIELDS = [\n",
    "    \"nodes\",\n",
    "    \"cpus_per_node\",\n",
    "    \"backends_per_node\",\n",
    "    \"dataclay\",\n",
    "    \"use_split\",\n",
    "]\n",
    "\n",
    "STD_VALUE_THRESHOLD = 2\n",
    "VIOLIN_BW = 0.1\n",
    "\n",
    "ESTIMATOR_TO_USE = np.mean\n",
    "#from functools import partial\n",
    "#ESTIMATOR_TO_USE = partial(np.percentile, q=25)\n",
    "\n",
    "HANDPICKED_OUTLIERS = [\n",
    "    # A lot of those seem to be for the executions done during 15th june at 7am. \n",
    "    # 6am executions were still fine.\n",
    "    # Temperature issues? General jitter? Sysadmin updating the system?\n",
    "    # We may never know\n",
    "    23429588, 23424157, 23424125, 23505880, 23505871, 23505863, 23429586, 23505895, 23505868,\n",
    "    23505904, 23505912, 23505886, 23505860,\n",
    "    23505887, 23505892, 23505915, 23505913, 23505864,\n",
    "    23503326, 23503411, 23429587, 23505920, 23505865,\n",
    "]\n",
    "\n",
    "def plot_things(data, estimator=ESTIMATOR_TO_USE):\n",
    "    sns.barplot(data=data, x=\"nodes\", hue=\"mode\", y=\"iteration_time\", \n",
    "                estimator=estimator)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "    plt.title(\"All\")\n",
    "    plt.show()\n",
    "\n",
    "#     fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12,10))\n",
    "    \n",
    "#     sns.barplot(data=data.query(\"dataclay == 0\"), x=\"nodes\", hue=\"mode\", y=\"iteration_time\", \n",
    "#                 palette=\"Set2\", estimator=estimator, ax=ax1)\n",
    "\n",
    "#     sns.violinplot(data=data.query(\"dataclay == 0\"), x=\"nodes\", hue=\"mode\", y=\"iteration_time\", \n",
    "#                    scale='width',\n",
    "#                    palette=\"Set2\", split=True, ax=ax2, inner=\"quartile\", bw=VIOLIN_BW)\n",
    "\n",
    "#     ax1.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "#     ax2.get_legend().remove()\n",
    "#     plt.suptitle(\"COMPSs executions\")\n",
    "#     plt.show()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12,10))\n",
    "\n",
    "    sns.barplot(data=data.query(\"dataclay == 1\"), x=\"nodes\", hue=\"mode\", y=\"iteration_time\",\n",
    "                estimator=estimator, ax=ax1, palette=\"Set2\")\n",
    "    sns.violinplot(data=data.query(\"dataclay == 1\"), x=\"nodes\", hue=\"mode\", y=\"iteration_time\", \n",
    "                   scale='width',\n",
    "                   split=True, ax=ax2, inner=\"quartile\", bw=VIOLIN_BW, palette=\"Set2\")\n",
    "    \n",
    "    ax1.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "    ax2.get_legend().remove()\n",
    "    plt.suptitle(\"dataClay executions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_mean(row):\n",
    "    row[\"fragments_per_node\"] = row[\"number_of_fragments\"] / (row[\"nodes\"] - 1)\n",
    "    if not row['dataclay']:\n",
    "        row['mode'] = \"COMPSs\"\n",
    "        if row[\"tracing\"] == 1.0:\n",
    "            row['mode'] += \" (tracing)\"\n",
    "    elif row['use_split']:\n",
    "        row['mode'] = \"dC+split\"\n",
    "    else:\n",
    "        row['mode'] = \"dC\"\n",
    "    \n",
    "    \n",
    "    if not row['compss_scheduler'] or not row['compss_working_dir']:\n",
    "        row[\"iteration_time\"] = np.nan\n",
    "        return row\n",
    "    \n",
    "    compss_settings_suffix = f\" [{row['compss_scheduler'].rsplit('.', 1)[-1]}/{row['compss_working_dir']}]\"\n",
    "    \n",
    "    row['mode'] += compss_settings_suffix\n",
    "    return row\n",
    "\n",
    "# Let's keep only those executions that \"make sense\"\n",
    "# (dataClay makes sense with FIFODataLocationScheduler and local_disk)\n",
    "# (when there is no dataClay, both schedulers make sense, but location should be done with local_disk)\n",
    "query_compss_gpfs = \"((dataclay == 0) and (compss_scheduler == 'es.bsc.compss.scheduler.fifodatanew.FIFODataScheduler') and (compss_working_dir == 'gpfs'))\"\n",
    "query_compss_ld = \"((dataclay == 0) and (compss_scheduler == 'es.bsc.compss.scheduler.fifodatalocation.FIFODataLocationScheduler') and (compss_working_dir == 'local_disk'))\"\n",
    "query_dataclay = \"((dataclay == 1) and (compss_scheduler == 'es.bsc.compss.scheduler.fifodatalocation.FIFODataLocationScheduler') and (compss_working_dir == 'local_disk'))\"\n",
    "\n",
    "db = DataStorage(\"kmeans-split\")\n",
    "df = db.to_dataframe().query(\"start_ts > '2022-06-10'\").query(\"tracing == 0\")\n",
    "\n",
    "df = df[df.id.isin(HANDPICKED_OUTLIERS) == False]\n",
    "\n",
    "# Remove old split experiments which had a bogus _multiplicity_ and \n",
    "# were inefficient\n",
    "to_drop = df.query(\"(start_ts < '2022-04-14') and (use_split == 1)\").index\n",
    "df = df.drop(to_drop).query(f\"{query_compss_gpfs} or {query_dataclay}\").apply(smart_mean, axis=1)\n",
    "#df = db.to_dataframe().apply(smart_mean, axis=1).query(\"(start_ts > '2021-02-18 20') and (start_ts < '2021-02-20')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = df.explode(\"iteration_time\").sort_values('mode')\n",
    "edf['iteration_time'] = edf['iteration_time'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7a108",
   "metadata": {},
   "source": [
    "# Weak scaling (small blocks)\n",
    "\n",
    "- 2304 fragments **per node**\n",
    "- 128000 points per fragment\n",
    "\n",
    "This shows the behaviour of split in a typical scalability environment. The quantity of objects is high and increases with the number of nodes. The benefits of the split should be more apparent as the number of nodes / work increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd364490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak scaling\n",
    "data = edf.query(\"((number_of_fragments / (nodes - 1)) == 2304) and (points_per_fragment == 64000)\")\n",
    "\n",
    "plot_things(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956e2d6",
   "metadata": {},
   "source": [
    "# Weak scaling (big blocks)\n",
    "\n",
    "- 48 fragments **per node**\n",
    "- Blocks are big\n",
    "\n",
    "This experiment has the same size as the previous one. \n",
    "\n",
    "This is a bad scenario for the split; data is perfectly balanced, so there is no real benefit of doing a split. Because the job load is high, the overhead may not be extremely big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak scaling, with big blocks (48 blocks per node)\n",
    "data = edf.query(\"((number_of_fragments / (nodes - 1)) == 48) and ((points_per_fragment) == (64000 * 48))\")\n",
    "\n",
    "plot_things(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02187efc",
   "metadata": {},
   "source": [
    "## Blocksize sweep\n",
    "\n",
    "8 worker nodes, analyze multiple block sizes (from 128000 points per block to 48 blocks per node). Those are the scenarios of weak scaling (see previous experiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak scaling, with big blocks (48 blocks per node)\n",
    "data = edf.query(\"(nodes == 9) and ((number_of_fragments * points_per_fragment) == 1179648000)\")\n",
    "\n",
    "def eval_granularity_index(row):\n",
    "    row[\"granularity_index\"] = row[\"number_of_fragments\"] // (48 * 8)\n",
    "    return row\n",
    "\n",
    "data = data.apply(eval_granularity_index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=data, x=\"granularity_index\", hue=\"mode\", y=\"iteration_time\", \n",
    "            estimator=ESTIMATOR_TO_USE)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.title(\"All\")\n",
    "# TODO: Change granularity_index to: fragments per core [o algo aixÃ­]\n",
    "plt.show()\n",
    "\n",
    "for i in [1, 4, 16, 48]:\n",
    "    ax = sns.violinplot(data=data.query(\"granularity_index == %d\" % i),\n",
    "                        x=\"granularity_index\", hue=\"mode\", y=\"iteration_time\", \n",
    "                        scale='width', bw=VIOLIN_BW,\n",
    "                        inner=\"quartile\", figsize=123)\n",
    "    ax.get_legend().remove()\n",
    "    plt.show()\n",
    "\n",
    "sns.barplot(data=data.query(\"dataclay == 1\"), x=\"granularity_index\", hue=\"mode\", y=\"iteration_time\", \n",
    "            estimator=ESTIMATOR_TO_USE)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.title(\"All\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb1627",
   "metadata": {},
   "source": [
    "## Split overhead\n",
    "\n",
    "**Work in progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5260c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some low-number-of-fragments split overhead are off-the-charts\n",
    "# and give some distorted outlier-full information\n",
    "data = data.query(\"split_time < 100\")\n",
    "\n",
    "sns.barplot(data=data.query(\"dataclay == 1\"), x=\"number_of_fragments\", y=\"split_time\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()\n",
    "\n",
    "sns.violinplot(data=data.query(\"dataclay == 1\"), x=\"number_of_fragments\", y=\"split_time\", \n",
    "               scale='width',\n",
    "               inner=\"quartile\", bw=VIOLIN_BW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4e5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
